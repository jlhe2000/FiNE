# FiNE: Filtering and Improving Noisy Data Elaborately with Large Language Models

[![Paper â€” NAACL 2025](https://img.shields.io/badge/Paper-NAACL%202025-blue)](https://aclanthology.org/2025.naacl-long.437/)
[![DOI](https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2025.naacl--long.437-1f6feb)](https://doi.org/10.18653/v1/2025.naacl-long.437)
[![Status](https://img.shields.io/badge/Status-Code%20Release%20in%20Progress-orange)](#-repository-status)
[![License](https://img.shields.io/badge/License-Apache--2.0-blue)](LICENSE)

Official repository for the NAACL 2025 long paper:

FiNE: Filtering and Improving Noisy Data Elaborately with Large Language Models  
Authors: Junliang He, Ziyue Fan, Shaohui Kuang, Li Xiaoqing, Kai Song, Yaqian Zhou, Xipeng Qiu  
DOI: 10.18653/v1/2025.naacl-long.437

- Paper page: https://aclanthology.org/2025.naacl-long.437/
- PDF: https://aclanthology.org/2025.naacl-long.437.pdf

---

## Overview

Large Language Models (LLMs) are powered by dataâ€”but open-source instruction datasets are often noisy, inconsistent, or misaligned with user preferences. FiNE is a method that leverages LLMs to perform refined filtering and improvement of training data. Rather than relying on coarse heuristics or one-off augmentation, FiNE orchestrates LLM-driven checks and edits that elevate data integrity at scale.

In experiments training Yi-1.5-9B, data processed with FiNE significantly closes the gap to the open-source chat version on AlignBench (from 1.08 to 0.35) and achieves +8.45 on HalluQA compared to the open-source chat baseline.

### Why FiNE

- LLM-driven, fine-grained data refinement for instruction datasets.
- Practical pipeline: combine filtering with targeted improvement rather than either alone.
- Demonstrated gains on downstream benchmarks (AlignBench, HalluQA) with Yi-1.5-9B.

---

## What's inside this repository

This repo serves as the public home for FiNE:

- Paper links and citation information.
- Release point for code, scripts, and data resources (including FiNE-related instruction datasets).
- FiNE-Alpaca is now open-sourced under `./FiNE-alpaca/`.

We are preparing cleaned datasets, reproduction scripts, and documentation to help you apply FiNE to your own data pipelines. See the status and roadmap below.

---

## FiNE-Alpaca

FiNE-Alpaca is our open-sourced resource that applies the FiNE methodology to Alpaca-style instruction data. It aims to provide higher-quality training data and reusable prompts/pipeline components. Please visit `./FiNE-alpaca/` for code and resources. Usage commands will be added as they become availableâ€”refer to that directory for notes and updates.

---

## Getting started

Core FiNE pipeline code and additional datasets are being organized for release. FiNE-Alpaca is available now at `./FiNE-alpaca/`. In the meantime:

- Read the paper for the full method and experimental setup:
  - PDF: https://aclanthology.org/2025.naacl-long.437.pdf
- Watch this repository to be notified when code and datasets are published.
- Open an issue if you have questions about applying FiNE in your setting or want early guidance.

### Anticipated contents (coming soon)

- FiNE pipeline scripts and configuration.
- Data validation and LLM-based refinement components.
- Example recipes for cleaning and improving common instruction datasets.
- Reproduction notes for Yi-1.5-9B experiments (AlignBench, HalluQA).

---

## Reproducibility notes

- Benchmarks: AlignBench, HalluQA.
- Model: Yi-1.5-9B (open-source chat version as baseline; FiNE-improved training data for our model).
- Metrics and comparisons follow the paperâ€™s protocol. Detailed scripts and exact data versions will be released here.

---

## Citation

If you find FiNE helpful, please cite our NAACL 2025 paper:

```bibtex
@inproceedings{he-etal-2025-fine,
    title = "{F}i{NE}: Filtering and Improving Noisy Data Elaborately with Large Language Models",
    author = "He, Junliang  and
      Fan, Ziyue  and
      Kuang, Shaohui  and
      Xiaoqing, Li  and
      Song, Kai  and
      Zhou, Yaqian  and
      Qiu, Xipeng",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.437/",
    doi = "10.18653/v1/2025.naacl-long.437",
    pages = "8686--8707",
    ISBN = "979-8-89176-189-6",
    abstract = "Data is the lifeblood of large language models (LLMs). While the quantity of open-source data available for training LLMs is substantial, its integrity often falls short. For instance, the open-source chat version of Yi-1.5-9B scores 5.20 on AlignBench, while the Chinese Alpaca-GPT4 version scores 4.12. This discrepancy makes it challenging for developers to create models that excel in downstream tasks and instruction following. Therefore, it is essential to improve data integrity. Currently, there are two mainstream methods for enhancing data integrity: data filtering and data augmentation. Due to the labor-intensive and time-consuming nature of performing these tasks manually, some of these efforts are now being undertaken by LLMs, owing to their high alignment with human preferences. However, we have found that performing data filtering or data augmentation with LLMs has limited effectiveness in improving data integrity. In this work, we propose FiNE (\\textbf{F}iltering and \\textbf{I}mproving \\textbf{N}oisy data \\textbf{E}laborately), a method that performs refined filtering and improvement of training data with LLMs. Using the data obtained through our method to train Yi-1.5-9B, the performance gap on AlignBench between our model and the open-source chat version is reduced from 1.08 to 0.35. Additionally, on HalluQA, our model surpasses the open-source chat version by 8.45."
}
```

---

## ä¸­æ–‡ç®€ä»‹

FiNE æ˜¯ä¸€ç§é¢å‘æŒ‡ä»¤æ•°æ®çš„ç²¾ç»†åŒ–â€œè¿‡æ»¤ + æ”¹è¿›â€æ–¹æ³•ï¼šå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ•°æ®è¿›è¡Œè´¨é‡ç­›æŸ¥ã€é—®é¢˜å®šä½ä¸æœ‰é’ˆå¯¹æ€§çš„ä¿®å¤ï¼Œä»è€Œç³»ç»Ÿæ€§æå‡æ•°æ®å®Œæ•´æ€§ä¸ä¸€è‡´æ€§ã€‚ä¸ä»…åšè¿‡æ»¤æˆ–ä»…åšå¢å¼ºç›¸æ¯”ï¼ŒFiNE é€šè¿‡è”åŠ¨çš„æµç¨‹æ›´æœ‰æ•ˆåœ°æå‡è®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§ã€‚

åœ¨ Yi-1.5-9B çš„è®­ç»ƒä¸­ï¼Œä½¿ç”¨ FiNE å¤„ç†åçš„æ•°æ®æ˜¾è‘—ç¼©å°ä¸å¼€æºèŠå¤©ç‰ˆåœ¨ AlignBench ä¸Šçš„å·®è·ï¼ˆä» 1.08 é™è‡³ 0.35ï¼‰ï¼Œå¹¶åœ¨ HalluQA ä¸Šç›¸å¯¹å¼€æºèŠå¤©ç‰ˆå–å¾— +8.45 çš„æå‡ã€‚

æˆ‘ä»¬æ­£åœ¨æ•´ç†ä»£ç ã€æ•°æ®ä¸å¤ç°è„šæœ¬ï¼Œå¹¶å°†åœ¨æœ¬ä»“åº“æŒç»­å‘å¸ƒæ›´æ–°ã€‚FiNE-Alpaca å·²å¼€æºï¼Œä»£ç ä¸èµ„æºä½äºæœ¬ä»“åº“çš„ `FiNE-alpaca/` ç›®å½•ã€‚

---

## ğŸ¤ Community & Support

- Questions or ideas? Please open an issue: https://github.com/jlhe2000/FiNE/issues
- Pull requests are welcome once the initial code release is available.

---

## ğŸ“Œ Repository Status

- Code release: in progress. This repository currently includes paper links and a directory for available/open-sourced resources.
- FiNE-Alpaca: open-sourced at `./FiNE-alpaca/`.
- Roadmap: dataset release and pipeline scripts are planned; watch the repo for updates.

---

## ğŸ“„ License

This repository is licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) for details.
